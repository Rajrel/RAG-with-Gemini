{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e6e2a22b-fb52-4219-b000-96fdb32d7ca2",
   "metadata": {},
   "source": [
    "## Query Translation\n",
    "Query translation is a series of steps to improve the likelihood of relevance between the query embeddings and the document embeddings to ensure the best possible match for the LLM answer generation.\n",
    "\n",
    "    -Rephrase\n",
    "    -Break-down\n",
    "    -Abstract\n",
    "    -Convert to Hypothetical Docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e6eeca23-0f41-49a3-b3d4-a97616b755f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display\n",
    "import os \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.getenv(\"GOOGLE_API_KEY\") \n",
    "my_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=my_api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de2b154-dd79-4ad3-9c2a-d21bda025ae2",
   "metadata": {},
   "source": [
    "### MultiQueryRetriever\n",
    "Distance-based vector database retrieval embeds (represents) queries in high-dimensional space and finds similar embedded documents based on \"distance\". But, retrieval may produce different results with subtle changes in query wording or if the embeddings do not capture the semantics of the data well. Prompt engineering / tuning is sometimes done to manually address these problems, but can be tedious.\n",
    "\n",
    "The MultiQueryRetriever automates the process of prompt tuning by using an LLM to generate multiple queries from different perspectives for a given user input query. For each query, it retrieves a set of relevant documents and takes the unique union across all queries to get a larger set of potentially relevant documents. By generating multiple perspectives on the same question, the MultiQueryRetriever might be able to overcome some of the limitations of the distance-based retrieval and get a richer set of results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f3345b-3d78-4860-a196-cd9037fb5092",
   "metadata": {},
   "source": [
    "##### INDEXING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfd6e1b0-61a8-46d9-a378-9bbd557584fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4d86546-f7a4-4784-8700-0fc53e980a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb00b9ef-a26d-47f0-aa63-0b17ed8392c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Index\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "## Call Embedding Model\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=embedding)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f31a40-c12c-410b-8292-ef846882e6d3",
   "metadata": {},
   "source": [
    "##### PROMPTING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6393abcb-9aab-4567-8eff-f8f437af9328",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='You are an AI language model assistant. Your task is to generate five \\ndifferent versions of the given user question to retrieve relevant documents from a vector \\ndatabase. By generating multiple perspectives on the user question, your goal is to help\\nthe user overcome some of the limitations of the distance-based similarity search. \\nProvide these alternative questions separated by newlines. Original question: {question}'))])\n",
       "| ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x00000242BE27A350>, async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x00000242BF850950>, default_metadata=())\n",
       "| StrOutputParser()\n",
       "| RunnableLambda(...)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Multi Query: Different Perspectives\n",
    "template = \"\"\"You are an AI language model assistant. Your task is to generate five \n",
    "different versions of the given user question to retrieve relevant documents from a vector \n",
    "database. By generating multiple perspectives on the user question, your goal is to help\n",
    "the user overcome some of the limitations of the distance-based similarity search. \n",
    "Provide these alternative questions separated by newlines. Original question: {question}\"\"\"\n",
    "prompt_perspectives = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(model= \"gemini-1.5-flash\", temperature = 0)\n",
    "\n",
    "generate_queries = (\n",
    "    prompt_perspectives \n",
    "    | llm\n",
    "    | StrOutputParser() \n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "generate_queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffd8351b-5032-4686-937c-c2f284976ae1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.load import dumps, loads\n",
    "\n",
    "def get_unique_union(documents: list[list]):\n",
    "    \"\"\" Unique union of retrieved docs \"\"\"\n",
    "    # Flatten list of lists, and convert each Document to string\n",
    "    flattened_docs = [dumps(doc) for sublist in documents for doc in sublist]\n",
    "    # Get unique documents\n",
    "    unique_docs = list(set(flattened_docs))\n",
    "    # Return\n",
    "    return [loads(doc) for doc in unique_docs]\n",
    "\n",
    "# Retrieve\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "retrieval_chain = generate_queries | retriever.map() | get_unique_union\n",
    "docs = retrieval_chain.invoke({\"question\":question})\n",
    "len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "27dfd7e9-780f-44ef-b3df-0698ae7dbb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down large, complex tasks into smaller, more manageable subgoals for LLM agents. This allows the agent to handle complex tasks more efficiently. \n",
      "\n",
      "The document mentions several methods for task decomposition:\n",
      "\n",
      "* **LLM with simple prompting:**  The LLM is prompted with questions like \"Steps for XYZ?\" or \"What are the subgoals for achieving XYZ?\"\n",
      "* **Task-specific instructions:**  The agent is given instructions tailored to the specific task, such as \"Write a story outline\" for writing a novel.\n",
      "* **Human inputs:**  Humans can provide the initial task decomposition, guiding the agent's planning.\n",
      "\n",
      "The document also highlights techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) which help LLMs decompose tasks into smaller steps. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "\n",
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    {\"context\": retrieval_chain, \n",
    "     \"question\": itemgetter(\"question\")} \n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "res = final_rag_chain.invoke({\"question\":question})\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af2a851-8530-41d7-bc13-b2ce30d70f0f",
   "metadata": {},
   "source": [
    "##### Drawbacks of the method \n",
    "\n",
    " - Human evaluation is a must to to understand the maximum number of chunks to use, the maximum number of queries to generate, and a response synthesizer function designed, among other activities, to make the most of the returned contexts.\n",
    " - This approach requires the user to design a clear prompt, leading subsequently to sub-query creation, making the process more reliant on the general-info pre-tuned LLM. This might not be able to understand the context of niche user queries.\n",
    " - Series of retrievals/generations triggers higher cost and latency\n",
    " - Having a broader contextual window to look for and giving equal weightage to all the contexts will degrade the model performance if not all the returned information is relevant to that question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0c98fff-b324-46ca-90e3-3f27e7c6a7c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
