{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "036931fc-447e-4eab-ae09-15a6c3505b31",
   "metadata": {},
   "source": [
    "## Multi-representation Indexing\n",
    "\n",
    "Problem: Many RAG approaches focus on splitting documents into chunks and retrieving some number based on similarity to an input question for the LLM. But chunk size and chunk number can be difficult to set and affect results if they do not provide full context for the LLM to answer a question.\n",
    "\n",
    "ðŸ’¡Idea: Proposition indexing (Tong Chen et al) is a nice paper that uses an LLM to produce document summaries (\"propositions\") that are optimized for retrieval. We've built on this with two retrievers.\n",
    "\n",
    "(1) multi-vector retriever embeds summaries, but returns full documents to the LLM. \n",
    "(2) parent-doc retriever embeds chunks but also returns full documents to the LLM. \n",
    "\n",
    "The idea is to get best of both worlds: use concise representations (summaries or chunks) that are well-suited to retrieval, but link them to documents, which have full context for generation.\n",
    "\n",
    "The approach is general, and can also be applied to tables or images: create / index a summary, but return the raw table or raw image for LLM generation. This gets around challenges w/ directly embedding tables (they can be split by poorly tuned chunk size) or images (need multi-modal embeddings), using a summary as a representation for text-based similarity search.\n",
    "\n",
    "[Related Idea](https://python.langchain.com/v0.1/docs/modules/data_connection/retrievers/parent_document_retriever)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a4dab31-e349-4333-ac23-aeab746ba190",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display\n",
    "import os \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.getenv(\"GOOGLE_API_KEY\") \n",
    "my_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=my_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b115ab59-36fd-4461-8a44-9adf40893b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import HypotheticalDocumentEmbedder, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "\n",
    "## Call Embedding Model\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(model= \"gemini-1.5-flash\", temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c289b271-b6d7-4ba7-b9fc-42588e079d9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "docs = loader.load()\n",
    "\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2024-02-05-human-data-quality/\")\n",
    "docs.extend(loader.load())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47dea9ff-709e-4f00-90ab-af9b79dcf189",
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "chain = (\n",
    "    {\"doc\": lambda x: x.page_content}\n",
    "    | ChatPromptTemplate.from_template(\"Summarize the following document:\\n\\n{doc}\")\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "summaries = chain.batch(docs, {\"max_concurrency\": 5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b3e2cbb-2e71-402f-a83e-26bbdd3fb571",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.storage import InMemoryByteStore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"summaries\",\n",
    "                     embedding_function=embedding\n",
    "                    )\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryByteStore()\n",
    "id_key = \"doc_id\"\n",
    "\n",
    "# The retriever\n",
    "retriever = MultiVectorRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    byte_store=store,\n",
    "    id_key=id_key,\n",
    ")\n",
    "doc_ids = [str(uuid.uuid4()) for _ in docs]\n",
    "\n",
    "# Docs linked to summaries\n",
    "summary_docs = [\n",
    "    Document(page_content=s, metadata={id_key: doc_ids[i]})\n",
    "    for i, s in enumerate(summaries)\n",
    "]\n",
    "\n",
    "# Add\n",
    "retriever.vectorstore.add_documents(summary_docs)\n",
    "retriever.docstore.mset(list(zip(doc_ids, docs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b1121dc-b820-4e8c-aec6-7879713162ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(page_content='This document explores the concept of LLM-powered autonomous agents, which are systems that use large language models (LLMs) as their core controllers. It delves into the key components of these agents, including planning, memory, and tool use.\\n\\n**Planning:**\\n\\n* **Task Decomposition:** LLMs can break down complex tasks into smaller, manageable subgoals using techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT).\\n* **Self-Reflection:** Agents can learn from past mistakes and refine their actions through self-criticism and self-reflection. Frameworks like ReAct, Reflexion, and Chain of Hindsight (CoH) enable this capability.\\n\\n**Memory:**\\n\\n* **Types of Memory:** The document draws parallels between human memory types (sensory, short-term, and long-term) and LLM capabilities.\\n* **Maximum Inner Product Search (MIPS):** External vector stores are used for long-term memory, enabling fast retrieval of information using approximate nearest neighbors (ANN) algorithms like LSH, ANNOY, HNSW, FAISS, and ScaNN.\\n\\n**Tool Use:**\\n\\n* **External APIs:** LLMs can be augmented with external tools and APIs to access information and capabilities beyond their pre-trained knowledge. Frameworks like MRKL, TALM, Toolformer, ChatGPT Plugins, and OpenAI API function calling demonstrate this.\\n* **HuggingGPT:** This framework uses ChatGPT as a task planner to select and execute models from the HuggingFace platform.\\n* **API-Bank:** This benchmark evaluates the performance of tool-augmented LLMs by providing a set of APIs and tasks.\\n\\n**Case Studies:**\\n\\n* **Scientific Discovery Agent:** ChemCrow uses LLMs and tools to perform tasks in organic synthesis, drug discovery, and materials design.\\n* **Generative Agents Simulation:** This experiment simulates virtual characters controlled by LLMs, demonstrating emergent social behavior.\\n\\n**Proof-of-Concept Examples:**\\n\\n* **AutoGPT:** This project showcases an autonomous agent that uses LLMs to achieve user-defined goals.\\n* **GPT-Engineer:** This project aims to generate code repositories based on natural language instructions.\\n\\n**Challenges:**\\n\\n* **Finite Context Length:** The limited context window of LLMs restricts their ability to process long sequences of information.\\n* **Long-Term Planning:** LLMs struggle with long-term planning and task decomposition, especially when dealing with unexpected errors.\\n* **Reliability of Natural Language Interface:** The reliance on natural language for communication can lead to errors and inconsistencies in agent behavior.\\n\\nThe document concludes by highlighting the potential of LLM-powered autonomous agents while acknowledging the challenges that need to be addressed for their widespread adoption.\\n', metadata={'doc_id': '6aad8245-f837-429a-9e4e-6f81dd50a0d9'})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"Memory in agents\"\n",
    "sub_docs = vectorstore.similarity_search(query,k=1)\n",
    "sub_docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e971e170-379c-44d3-82dc-23ad31fba03d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Number of requested results 4 is greater than number of elements in index 2, updating n_results = 2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"\\n\\n\\n\\n\\n\\nLLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\\n\\n\\n\\n\\nTags\\n\\n\\n\\n\\nFAQ\\n\\n\\n\\n\\nemojisearch.app\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n      LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\n \\n\\n\\nTable of Contents\\n\\n\\n\\nAgent System Overview\\n\\nComponent One: Planning\\n\\nTask Decomposition\\n\\nSelf-Reflection\\n\\n\\nComponent Two: Memory\\n\\nTypes of Memory\\n\\nMaximum Inner Product Search (MIPS)\\n\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retrieved_docs = retriever.invoke(query,n_results=1)\n",
    "retrieved_docs[0].page_content[0:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35ad79ad-cfd7-4e3c-828d-96c0762284c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
