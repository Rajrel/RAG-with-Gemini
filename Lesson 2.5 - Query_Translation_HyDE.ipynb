{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e06e218b-5ecc-4657-aa2b-e5cb6b4462ff",
   "metadata": {},
   "source": [
    "### Hypothetical Document Embeddings (HyDE)\n",
    "At a high level, HyDE is an embedding technique that takes queries, generates a hypothetical answer, and then embeds that generated document and uses that as the final example.\n",
    "\n",
    "In order to use HyDE, we therefore need to provide a base embedding model, as well as an LLMChain that can be used to generate those documents. By default, the HyDE class comes with some default prompts to use (see the paper for more details on them), but we can also create our own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c105542-f0fb-4bf4-9fcb-da49b58afed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display\n",
    "import os \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.getenv(\"GOOGLE_API_KEY\") \n",
    "my_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=my_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e941c1a6-f375-4bb6-b593-91aa5be29d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import HypotheticalDocumentEmbedder, LLMChain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "\n",
    "## Call Embedding Model\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(model= \"gemini-1.5-flash\", temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0e6748e7-5941-4a90-a650-b89d8160cf46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load with `web_search` prompt\n",
    "embeddings = HypotheticalDocumentEmbedder.from_llm(llm, embedding, \"web_search\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06d5c5e2-8439-4a54-8b8b-495b20ba4d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can use it as any embedding class!\n",
    "result = embeddings.embed_query(\"Where is the Taj Mahal?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1e0d2b46-039d-4d09-a253-278a01662263",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_text_splitters import CharacterTextSplitter\n",
    "\n",
    "with open(\"Data/state_of_the_union.txt\", encoding = \"utf-8\") as f:\n",
    "    state_of_the_union = f.read()\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "texts = text_splitter.split_text(state_of_the_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5142898f-2beb-40a1-8dad-6cdda93491d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "docsearch = Chroma.from_texts(texts, embeddings)\n",
    "\n",
    "query = \"What did the president say about Ketanji Brown Jackson\"\n",
    "docs = docsearch.similarity_search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebbd55e0-cc85-45e0-ab78-cac49a29d4ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tonight. I call on the Senate to: Pass the Freedom to Vote Act. Pass the John Lewis Voting Rights Act. And while you’re at it, pass the Disclose Act so Americans can know who is funding our elections. \n",
      "\n",
      "Tonight, I’d like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer—an Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service. \n",
      "\n",
      "One of the most serious constitutional responsibilities a President has is nominating someone to serve on the United States Supreme Court. \n",
      "\n",
      "And I did that 4 days ago, when I nominated Circuit Court of Appeals Judge Ketanji Brown Jackson. One of our nation’s top legal minds, who will continue Justice Breyer’s legacy of excellence.\n"
     ]
    }
   ],
   "source": [
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e8afc3-0245-410c-9110-e4bd787e77c2",
   "metadata": {},
   "source": [
    "##### RAG Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5ba031a6-e329-4b64-a98b-1b41e9877613",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f68ed82-41e5-4df8-bb25-1de5a6bb2ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "## Call Embedding Model\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=embedding)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "761d63d3-29ac-447a-bff4-0f7cc117b4b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## Task Decomposition for Large Language Model Agents\\n\\nTask decomposition is a crucial aspect of enabling Large Language Models (LLMs) to effectively act as agents in complex environments. It involves breaking down a high-level, often ambiguous task into a sequence of smaller, more manageable subtasks that the LLM can execute. This process is essential for several reasons:\\n\\n**1. Bridging the Gap between Language and Action:** LLMs excel at understanding and generating natural language, but they lack the ability to directly interact with the real world. Task decomposition allows us to translate complex, human-understandable instructions into a series of actions that the LLM can perform through APIs or other interfaces.\\n\\n**2. Handling Complexity and Uncertainty:** Real-world tasks are often multifaceted and involve uncertainty. Decomposing tasks into smaller steps allows the LLM to handle complexity by focusing on one subtask at a time. This also enables the agent to adapt to unforeseen circumstances by adjusting its plan based on feedback from the environment.\\n\\n**3. Enhancing Efficiency and Scalability:** By breaking down tasks into smaller units, task decomposition allows for parallel execution, potentially speeding up the overall process. This is particularly important for complex tasks that require multiple steps or involve interactions with multiple systems.\\n\\n**4. Facilitating Learning and Improvement:** Task decomposition provides a structured framework for LLMs to learn from their experiences. By analyzing the success or failure of each subtask, the agent can identify areas for improvement and refine its strategy for future tasks.\\n\\n**Methods for Task Decomposition:**\\n\\nSeveral approaches are employed for task decomposition in LLM agents, including:\\n\\n* **Rule-based decomposition:** This approach relies on predefined rules and heuristics to break down tasks into subtasks. While simple to implement, it can be inflexible and may not generalize well to new tasks.\\n* **Reinforcement learning:** LLMs can be trained to learn optimal task decomposition strategies through reinforcement learning. This approach allows for greater flexibility and adaptability but requires significant training data and computational resources.\\n* **Hybrid approaches:** Combining rule-based methods with reinforcement learning can leverage the strengths of both approaches, leading to more robust and efficient task decomposition strategies.\\n\\n**Conclusion:**\\n\\nTask decomposition is a critical component of enabling LLMs to act as agents in complex environments. By breaking down tasks into manageable subtasks, it allows LLMs to bridge the gap between language and action, handle complexity and uncertainty, enhance efficiency and scalability, and facilitate learning and improvement. As research in this area continues, we can expect to see increasingly sophisticated and effective task decomposition methods that empower LLMs to perform a wider range of tasks in the real world. \\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# HyDE document genration\n",
    "template = \"\"\"Please write a scientific paper passage to answer the question\n",
    "Question: {question}\n",
    "Passage:\"\"\"\n",
    "prompt_hyde = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(model= \"gemini-1.5-flash\", temperature = 0)\n",
    "\n",
    "generate_docs_for_retrieval = (\n",
    "    prompt_hyde | llm | StrOutputParser() \n",
    ")\n",
    "\n",
    "# Run\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_docs_for_retrieval.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bfed19bf-a507-48ec-bef6-da9f5b1b61d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(page_content='Fig. 1. Overview of a LLM-powered autonomous agent system.\\nComponent One: Planning#\\nA complicated task usually involves many steps. An agent needs to know what they are and plan ahead.\\nTask Decomposition#\\nChain of thought (CoT; Wei et al. 2022) has become a standard prompting technique for enhancing model performance on complex tasks. The model is instructed to “think step by step” to utilize more test-time computation to decompose hard tasks into smaller and simpler steps. CoT transforms big tasks into multiple manageable tasks and shed lights into an interpretation of the model’s thinking process.\\nTree of Thoughts (Yao et al. 2023) extends CoT by exploring multiple reasoning possibilities at each step. It first decomposes the problem into multiple thought steps and generates multiple thoughts per step, creating a tree structure. The search process can be BFS (breadth-first search) or DFS (depth-first search) with each state evaluated by a classifier (via a prompt) or majority vote.\\nTask decomposition can be done (1) by LLM with simple prompting like \"Steps for XYZ.\\\\n1.\", \"What are the subgoals for achieving XYZ?\", (2) by using task-specific instructions; e.g. \"Write a story outline.\" for writing a novel, or (3) with human inputs.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='LLM Powered Autonomous Agents\\n    \\nDate: June 23, 2023  |  Estimated Reading Time: 31 min  |  Author: Lilian Weng\\n\\n\\nBuilding agents with LLM (large language model) as its core controller is a cool concept. Several proof-of-concepts demos, such as AutoGPT, GPT-Engineer and BabyAGI, serve as inspiring examples. The potentiality of LLM extends beyond generating well-written copies, stories, essays and programs; it can be framed as a powerful general problem solver.\\nAgent System Overview#\\nIn a LLM-powered autonomous agent system, LLM functions as the agent’s brain, complemented by several key components:\\n\\nPlanning\\n\\nSubgoal and decomposition: The agent breaks down large tasks into smaller, manageable subgoals, enabling efficient handling of complex tasks.\\nReflection and refinement: The agent can do self-criticism and self-reflection over past actions, learn from mistakes and refine them for future steps, thereby improving the quality of final results.\\n\\n\\nMemory', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Another quite distinct approach, LLM+P (Liu et al. 2023), involves relying on an external classical planner to do long-horizon planning. This approach utilizes the Planning Domain Definition Language (PDDL) as an intermediate interface to describe the planning problem. In this process, LLM (1) translates the problem into “Problem PDDL”, then (2) requests a classical planner to generate a PDDL plan based on an existing “Domain PDDL”, and finally (3) translates the PDDL plan back into natural language. Essentially, the planning step is outsourced to an external tool, assuming the availability of domain-specific PDDL and a suitable planner which is common in certain robotic setups but not in many other domains.\\nSelf-Reflection#\\nSelf-reflection is a vital aspect that allows autonomous agents to improve iteratively by refining past action decisions and correcting previous mistakes. It plays a crucial role in real-world tasks where trial and error are inevitable.\\nReAct (Yao et al. 2023) integrates reasoning and acting within LLM by extending the action space to be a combination of task-specific discrete actions and the language space. The former enables LLM to interact with the environment (e.g. use Wikipedia search API), while the latter prompting LLM to generate reasoning traces in natural language.\\nThe ReAct prompt template incorporates explicit steps for LLM to think, roughly formatted as:', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'}),\n",
       " Document(page_content='Finite context length: The restricted context capacity limits the inclusion of historical information, detailed instructions, API call context, and responses. The design of the system has to work with this limited communication bandwidth, while mechanisms like self-reflection to learn from past mistakes would benefit a lot from long or infinite context windows. Although vector stores and retrieval can provide access to a larger knowledge pool, their representation power is not as powerful as full attention.\\n\\n\\nChallenges in long-term planning and task decomposition: Planning over a lengthy history and effectively exploring the solution space remain challenging. LLMs struggle to adjust plans when faced with unexpected errors, making them less robust compared to humans who learn from trial and error.\\n\\n\\nReliability of natural language interface: Current agent system relies on natural language as an interface between LLMs and external components such as memory and tools. However, the reliability of model outputs is questionable, as LLMs may make formatting errors and occasionally exhibit rebellious behavior (e.g. refuse to follow an instruction). Consequently, much of the agent demo code focuses on parsing model output.\\n\\n\\nCitation#\\nCited as:\\n\\nWeng, Lilian. (Jun 2023). “LLM-powered Autonomous Agents”. Lil’Log. https://lilianweng.github.io/posts/2023-06-23-agent/.', metadata={'source': 'https://lilianweng.github.io/posts/2023-06-23-agent/'})]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Retrieve\n",
    "retrieval_chain = generate_docs_for_retrieval | retriever \n",
    "retireved_docs = retrieval_chain.invoke({\"question\":question})\n",
    "retireved_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e5837f60-8d0c-4db1-acc9-c66e3c3c9f39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG\n",
    "template = \"\"\"Answer the following question based on this context:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "res = final_rag_chain.invoke({\"context\":retireved_docs,\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c93ba51b-ac82-4a87-8c3c-4ff6981fbcd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition for LLM agents is the process of breaking down large, complex tasks into smaller, more manageable subgoals. This allows the agent to handle complex tasks more efficiently. \n",
      "\n",
      "The document highlights three methods for task decomposition:\n",
      "\n",
      "1. **LLM with simple prompting:**  The LLM is prompted to list steps or subgoals for a given task.\n",
      "2. **Task-specific instructions:**  The agent is given instructions tailored to the specific task, such as \"Write a story outline\" for writing a novel.\n",
      "3. **Human inputs:**  Humans can provide the initial task decomposition, guiding the agent's planning process.\n",
      "\n",
      "The document also mentions two specific techniques for task decomposition:\n",
      "\n",
      "* **Chain of Thought (CoT):**  The LLM is instructed to \"think step by step\" to break down complex tasks into smaller steps.\n",
      "* **Tree of Thoughts (ToT):**  This extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of potential solutions. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227df109-9ba9-4b8c-b013-510d9dc88966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
