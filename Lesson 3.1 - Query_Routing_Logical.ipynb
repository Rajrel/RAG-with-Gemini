{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f48760-fad0-46f1-b012-fb3d8ca25694",
   "metadata": {},
   "source": [
    "## Routing\n",
    "<b> Sometimes we have multiple indexes for different domains </b>, and for different questions we want to query different subsets of these indexes. For example, suppose we had one vector store index for all of the LangChain python documentation and one for all of the LangChain js documentation. Given a question about LangChain usage, we'd want to infer which language the the question was referring to and query the appropriate docs. <b> Query routing is the process of classifying which index or subset of indexes a query should be performed on </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f2972d-d74e-4105-b374-0abfd1606bdf",
   "metadata": {},
   "source": [
    "### Logical Routing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19942522-3621-415e-bf9c-3697d709de86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display\n",
    "import os \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.getenv(\"GOOGLE_API_KEY\") \n",
    "my_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=my_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7076494a-962e-4bff-b8a5-cd04f174203f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "\n",
    "## Call Embedding Model\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(model= \"gemini-1.5-flash\", temperature = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6a9b5758-3a4f-4c29-92ea-0aeabca31908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Literal\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.pydantic_v1 import BaseModel, Field\n",
    "\n",
    "# Data model\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Route a user query to the most relevant datasource.\"\"\"\n",
    "\n",
    "    datasource: Literal[\"python\", \"js\", \"golang\"] = Field(\n",
    "        ...,\n",
    "        description=\"Given a user question choose which datasource would be most relevant for answering their question\",\n",
    "    )\n",
    "\n",
    "# LLM with function call \n",
    "structured_llm = llm.with_structured_output(RouteQuery)\n",
    "\n",
    "# Prompt \n",
    "system = \"\"\"You are an expert at routing a user question to the appropriate data source.\n",
    "\n",
    "Based on the programming language the question is referring to, route it to the relevant data source.\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", system),\n",
    "        (\"human\", \"{question}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define router \n",
    "router = prompt | structured_llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4510f185-3990-43ae-91dd-76bb52220e11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptTemplate(input_variables=['question'], messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], template='You are an expert at routing a user question to the appropriate data source.\\n\\nBased on the programming language the question is referring to, route it to the relevant data source.')), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['question'], template='{question}'))])\n",
       "| RunnableBinding(bound=ChatGoogleGenerativeAI(model='models/gemini-1.5-flash', temperature=0.0, client=<google.ai.generativelanguage_v1beta.services.generative_service.client.GenerativeServiceClient object at 0x00000262BE897090>, async_client=<google.ai.generativelanguage_v1beta.services.generative_service.async_client.GenerativeServiceAsyncClient object at 0x00000262BE896BD0>, default_metadata=()), kwargs={'tools': [{'function_declarations': [{'name': 'RouteQuery', 'description': 'Route a user query to the most relevant datasource.', 'parameters': {'type': 'object', 'properties': {'datasource': {'type': 'string', 'description': 'Given a user question choose which datasource would be most relevant for answering their question'}}, 'required': ['datasource']}}]}], 'tool_config': None})\n",
       "| PydanticToolsParser(first_tool_only=True, tools=[<class '__main__.RouteQuery'>])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "router"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46e2214-773c-4e41-b3cb-0f2aabdd6e50",
   "metadata": {},
   "source": [
    "<img src=\"images/structured_output.png\" alt=\"llm.structured\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "518abc69-38a7-43c9-a163-f2ea8d17ef6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"\"\"Why doesn't the following code work:\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\"human\", \"speak in {language}\"])\n",
    "prompt.invoke(\"english\")\n",
    "\"\"\"\n",
    "\n",
    "result = router.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ecab430b-30db-4df3-9f27-1c2c115ebd0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RouteQuery(datasource='python')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0cf75179-3052-49cd-8d75-fd93d84d3a48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'python'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.datasource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fad532f9-46d3-44ff-92bc-2864dfd2ac4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_route(result):\n",
    "    if \"python\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for python\"\n",
    "    elif \"js\" in result.datasource.lower():\n",
    "        ### Logic here \n",
    "        return \"chain for js\"\n",
    "    else:\n",
    "        ### Logic here \n",
    "        return \"golang\"\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "full_chain = router | RunnableLambda(choose_route)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5430cfe5-0278-484e-ba49-c95f111e8763",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'chain for python'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70e57f6-487a-4215-9426-256a088ae3d7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
