{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "727cbe97-0e1e-4cf5-a3fa-892c26490052",
   "metadata": {},
   "source": [
    "## Retrieval augmented generation (RAG)\n",
    "Fine-tuning an LLM with specific facts is one way to mitigate this, but is often poorly suited for factual recall and can be costly. Retrieval is the process of providing relevant information to an LLM to improve its response for a given input. Retrieval augmented generation (RAG) is the process of grounding the LLM generation (output) using the retrieved information.\n",
    "\n",
    "<img src=\"images/RAG_Landscape.png\" alt=\"Retrival Augumented Generation (RAG Landscape)\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ab9d548-784f-40b5-ad90-52278b2827f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display\n",
    "import os \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.getenv(\"GOOGLE_API_KEY\") \n",
    "my_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=my_api_key)\n",
    "langchain_api_key = os.getenv('LANGSMITH_API_KEY')\n",
    "\n",
    "os.environ['LANGCHAIN_TRACING_V2'] = 'true'\n",
    "os.environ['LANGCHAIN_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
    "os.environ['LANGCHAIN_API_KEY'] = langchain_api_key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cbc899-6790-4161-ac74-9bc4a823a216",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchainhub -qU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5139af5-1a1a-4c4d-8903-06311dffbe45",
   "metadata": {},
   "source": [
    "### Indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa386fd8-006e-4ffc-b340-64ca71044d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI, GoogleGenerativeAIEmbeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a5a0f2-a8a8-43b7-a48b-90637a71786a",
   "metadata": {},
   "source": [
    "##### Document Loaders "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3a843a8b-ec6c-48eb-a311-73074a20e603",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Documents\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd561428-16ac-49b5-acf9-37505a8d436a",
   "metadata": {},
   "source": [
    "##### Splitters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "743af447-2aac-4c3e-874d-390d46b384a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000,\n",
    "                                               chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6964b0c0-fc9a-42ae-95f1-13c0b459a647",
   "metadata": {},
   "source": [
    "##### Vectorstores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "20c16f94-704b-4d17-b96f-f8fb18a04d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Call Embedding Model\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "\n",
    "# Embed/Index\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=embedding )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbfcf4ab-c69d-4483-a5bf-1fc3399e7a6f",
   "metadata": {},
   "source": [
    "### Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fd7a58d4-1aee-4ed6-aea9-be05a66b7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 1})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbe1184-4de3-4300-b419-c60e9baaa7cb",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d5aa6a72-3124-4a7e-ba79-00c571021423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "prompt_hub_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "# LLM\n",
    "llm = llm = ChatGoogleGenerativeAI(model= \"gemini-1.5-flash\", temperature = 0)\n",
    "# Post-processing\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279903a8-70c4-4903-abbe-3c99cd9c86cf",
   "metadata": {},
   "source": [
    "### Chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f953e062-f38c-47cd-a62b-dc9aeb71d799",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19deabb9-646b-4f64-9a11-4b7b7f2f7fcd",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a1b83f79-fbda-4b13-acf0-88472757252a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is the process of breaking down a complex task into smaller, more manageable steps. This is often achieved through the use of chain of thought prompting, which encourages the model to think step-by-step and decompose the task into simpler subtasks. This approach helps to improve model performance on complex tasks and provides insights into the model's reasoning process. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = rag_chain.invoke(\"What is Task Decomposition?\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c4b81-09cf-4a11-a024-aa310b0fd5e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
