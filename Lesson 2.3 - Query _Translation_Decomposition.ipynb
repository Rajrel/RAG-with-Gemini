{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3095c730-dd6c-44df-ab01-67968c0081d1",
   "metadata": {},
   "source": [
    "### Decomposition\n",
    "\n",
    "This process of splitting an input into multiple distinct sub-queries is what we refer to as query decomposition. It is also sometimes referred to as sub-query generation. This simplifies the prompts and increases the context for the retrieval process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40935520-0216-426b-979f-44675f15641c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display\n",
    "import os \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.getenv(\"GOOGLE_API_KEY\") \n",
    "my_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=my_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2232787-c824-4629-8a92-e95e9d7a3130",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c4269c09-eea6-4585-b7f8-d094b81902f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "## Call Embedding Model\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=embedding)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93f7e404-be27-4f8b-93ec-eaafe523dc61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# Decomposition\n",
    "template = \"\"\"You are a helpful assistant that generates multiple sub-questions related to an input question. \\n\n",
    "The goal is to break down the input into a set of sub-problems / sub-questions that can be answers in isolation. \\n\n",
    "Generate multiple search queries related to: {question} \\n\n",
    "Output (3 queries):\"\"\"\n",
    "prompt_decomposition = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f655fbc6-5913-4c66-9a1f-734e24f52071",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(model= \"gemini-1.5-flash\", temperature = 0)\n",
    "\n",
    "# Chain\n",
    "generate_queries_decomposition = ( prompt_decomposition | llm | StrOutputParser() | (lambda x: x.split(\"\\n\")))\n",
    "\n",
    "# Run\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "questions = generate_queries_decomposition.invoke({\"question\":question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9fdc2bea-2a3c-4d14-800b-5eaa5266f11e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Here are three search queries related to \"What is task decomposition for LLM agents?\":',\n",
       " '',\n",
       " '1. **\"Task decomposition in large language model agents\"** - This query focuses on the general concept of task decomposition within the context of LLM agents.',\n",
       " '2. **\"How do LLMs decompose complex tasks into subtasks\"** - This query delves into the specific mechanisms LLMs use to break down complex tasks.',\n",
       " '3. **\"Benefits of task decomposition for LLM agent performance\"** - This query explores the advantages of using task decomposition for improving the effectiveness of LLM agents. ',\n",
       " '']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f842c72-a8a1-4241-8657-d198766c0d98",
   "metadata": {},
   "source": [
    "##### Answer recursively\n",
    "\n",
    "Here we are passing the questions one by one along with the previous Q-and-A response and context fetched for the current question. This, in turn, retains the old perspective and synchronizes the solution with the new perspective, making the solution more nuanced. This approach has proven to be effective against really complex queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c5355f73-8a9b-46b1-8b1d-d6c5da2fe8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prompt\n",
    "template = \"\"\"Here is the question you need to answer:\n",
    "\n",
    "\\n --- \\n {question} \\n --- \\n\n",
    "\n",
    "Here is any available background question + answer pairs:\n",
    "\n",
    "\\n --- \\n {q_a_pairs} \\n --- \\n\n",
    "\n",
    "Here is additional context relevant to the question: \n",
    "\n",
    "\\n --- \\n {context} \\n --- \\n\n",
    "\n",
    "Use the above context and any background question + answer pairs to answer the question: \\n {question}\n",
    "\"\"\"\n",
    "\n",
    "decomposition_prompt = ChatPromptTemplate.from_template(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5f32949-0b17-43e8-afeb-7969049b999f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import itemgetter\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "def format_qa_pair(question, answer):\n",
    "    \"\"\"Format Q and A pair\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    formatted_string += f\"Question: {question}\\nAnswer: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "q_a_pairs = \"\"\n",
    "for q in questions:\n",
    "    \n",
    "    rag_chain = (\n",
    "    {\"context\": itemgetter(\"question\") | retriever, \n",
    "     \"question\": itemgetter(\"question\"),\n",
    "     \"q_a_pairs\": itemgetter(\"q_a_pairs\")} \n",
    "    | decomposition_prompt\n",
    "    | llm\n",
    "    | StrOutputParser())\n",
    "\n",
    "    answer = rag_chain.invoke({\"question\":q,\"q_a_pairs\":q_a_pairs})\n",
    "    q_a_pair = format_qa_pair(q,answer)\n",
    "    q_a_pairs = q_a_pairs + \"\\n---\\n\"+  q_a_pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "503ba75d-448b-49df-b1ba-a41baf9e4895",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'## What is task decomposition for LLM agents?\\n\\nTask decomposition is a crucial technique for building effective and safe LLM agents. It involves breaking down complex tasks into smaller, manageable steps that the LLM can understand and execute. This process is essential for several reasons:\\n\\n**1. Handling Complexity:** LLMs are powerful language processors, but they struggle with complex tasks requiring multiple steps and reasoning. Task decomposition breaks down these challenges into smaller, manageable chunks, allowing LLMs to tackle them more effectively.\\n\\n**2. Improved Efficiency:** By dividing a task into smaller steps, LLMs can focus their processing power on each individual step, leading to more efficient execution. This is particularly important for tasks that require a lot of computational resources.\\n\\n**3. Enhanced Safety and Control:** Task decomposition allows for better control over the LLM\\'s actions. By breaking down a task into smaller steps, it becomes easier to identify potential risks and implement safeguards to mitigate them. This is crucial for ensuring that LLMs are used responsibly and ethically.\\n\\n**4. Improved Interpretability:** Task decomposition can make the LLM\\'s reasoning process more transparent. By breaking down a task into smaller steps, it becomes easier to understand how the LLM arrived at its final output. This can be helpful for debugging and improving the LLM\\'s performance.\\n\\n**5. Facilitates Learning and Adaptation:**  Breaking down tasks into smaller steps allows LLMs to learn from each step individually. This enables them to adapt to new situations and improve their performance over time.\\n\\n**6. Enables Planning and Goal-Oriented Behavior:** Task decomposition allows LLMs to plan their actions and work towards specific goals. This is essential for creating agents that can perform complex tasks autonomously.\\n\\n**Examples from the provided context:**\\n\\n* **HuggingGPT:** This agent uses an LLM to parse user requests into multiple tasks, each with specific attributes like type, ID, dependencies, and arguments. This demonstrates how LLMs can be used to plan and decompose tasks.\\n* **Generative Agents:** These agents use LLMs to simulate human behavior, often requiring complex task decomposition to achieve realistic interactions.\\n* **Anticancer Drug Discovery:** The example of the LLM agent attempting to synthesize a drug demonstrates how task decomposition can be used to break down a complex scientific process into smaller, manageable steps.\\n\\n**Techniques for Task Decomposition:**\\n\\nSeveral techniques are used for task decomposition in LLM agents:\\n\\n* **Chain of Thought (CoT):** This technique involves prompting the LLM to \"think step by step\" and break down a complex task into smaller, simpler steps.\\n* **Tree of Thoughts (ToT):** This technique extends CoT by exploring multiple reasoning possibilities at each step, creating a tree structure of potential solutions.\\n* **Task-Specific Instructions:** LLMs can be trained on specific tasks, allowing them to automatically decompose tasks into appropriate steps.\\n* **Human Input:** In some cases, human input can be used to guide the LLM\\'s task decomposition process.\\n\\n**Conclusion:**\\n\\nTask decomposition is a fundamental concept in the development of LLM agents. By breaking down complex tasks into smaller, manageable steps, LLMs can be empowered to tackle a wider range of challenges, improve their efficiency, enhance safety and control, and make their reasoning processes more transparent. As LLM agents become more sophisticated, task decomposition will continue to play a crucial role in their development and deployment. \\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eff71fa-8e62-4720-8fd5-4db5e1cbfa3e",
   "metadata": {},
   "source": [
    "##### Answer individually \n",
    "Also known as Parallel Answering Approach, in this approach, we are decomposing the user prompt into nuanced slices as before. The difference is that we are attempting to solve them in parallel. Here, we answer each question individually and then combine them together for a much more nuanced context, which is then used for answering the user query. Depending on the quality of the sub-queries, this approach is an efficient solution for most use cases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ece27456-3f93-4aa7-b2c1-24c5fb4f0c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer each sub-question individually \n",
    "\n",
    "from langchain import hub\n",
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "\n",
    "# RAG prompt\n",
    "prompt_rag = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "def retrieve_and_rag(question,prompt_rag,sub_question_generator_chain):\n",
    "    \"\"\"RAG on each sub-question\"\"\"\n",
    "    \n",
    "    # Use our decomposition / \n",
    "    sub_questions = sub_question_generator_chain.invoke({\"question\":question})\n",
    "    \n",
    "    # Initialize a list to hold RAG chain results\n",
    "    rag_results = []\n",
    "    \n",
    "    for sub_question in sub_questions:\n",
    "        \n",
    "        # Retrieve documents for each sub-question\n",
    "        retrieved_docs = retriever.invoke(sub_question)\n",
    "        \n",
    "        # Use retrieved documents and sub-question in RAG chain\n",
    "        answer = (prompt_rag | llm | StrOutputParser()).invoke({\"context\": retrieved_docs, \n",
    "                                                                \"question\": sub_question})\n",
    "        rag_results.append(answer)\n",
    "    \n",
    "    return rag_results,sub_questions\n",
    "\n",
    "# Wrap the retrieval and RAG process in a RunnableLambda for integration into a chain\n",
    "answers, questions = retrieve_and_rag(question, prompt_rag, generate_queries_decomposition)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d10801f-9355-440b-be49-1abad85d4b97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"## What is task decomposition for LLM agents?\\n\\nTask decomposition for LLM agents is the process of breaking down large, complex tasks into smaller, more manageable subgoals. This allows the agent to efficiently handle complex tasks and improve the quality of its final results. \\n\\nHere's how it works:\\n\\n* **LLMs can be prompted to decompose tasks using:**\\n    * **Simple instructions:**  The prompt can directly instruct the LLM to break down the task into steps.\\n    * **Task-specific instructions:**  The prompt can provide specific instructions tailored to the task, guiding the LLM on how to decompose it.\\n    * **Human input:**  Humans can provide explicit instructions on how to decompose the task, giving the LLM a clear roadmap.\\n\\n* **Techniques like Chain of Thought (CoT) and Tree of Thoughts (ToT) are used to decompose tasks into smaller steps:**\\n    * **CoT:**  Prompts the model to break down tasks into smaller steps, allowing it to reason and solve problems more effectively.\\n    * **ToT:**  Explores multiple reasoning possibilities at each step, creating a tree structure that helps the LLM consider different approaches and find the best solution.\\n\\n**Benefits of task decomposition for LLM agent performance:**\\n\\n* **Improved performance:**  Breaking down complex tasks into smaller steps makes it easier for the model to understand and execute the task, leading to better results.\\n* **Enhanced reasoning:**  Task decomposition provides a structured approach to problem-solving, enabling LLMs to reason more effectively.\\n\\nIn essence, task decomposition empowers LLM agents to tackle complex tasks by simplifying them into manageable chunks, ultimately leading to more efficient and accurate outcomes. \\n\""
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_qa_pairs(questions, answers):\n",
    "    \"\"\"Format Q and A pairs\"\"\"\n",
    "    \n",
    "    formatted_string = \"\"\n",
    "    for i, (question, answer) in enumerate(zip(questions, answers), start=1):\n",
    "        formatted_string += f\"Question {i}: {question}\\nAnswer {i}: {answer}\\n\\n\"\n",
    "    return formatted_string.strip()\n",
    "\n",
    "context = format_qa_pairs(questions, answers)\n",
    "\n",
    "# Prompt\n",
    "template = \"\"\"Here is a set of Q+A pairs:\n",
    "\n",
    "{context}\n",
    "\n",
    "Use these to synthesize an answer to the question: {question}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "final_rag_chain = (\n",
    "    prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "final_rag_chain.invoke({\"context\":context,\"question\":question})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e1de68-951d-46bf-baa0-8edb13d9134f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
