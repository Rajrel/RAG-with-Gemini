{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57970ea3-2ac0-4e9a-a7f2-c4e445d1e591",
   "metadata": {},
   "source": [
    "### Step Back Prompting\n",
    "\n",
    "The intuition behind step-back prompting is to encourage the LLM to think deeply and engage in a form of meta-reasoning before tackling a task. We generate a more abstract question from the user query to get a broader perspective of the task, retrieving more contexts. Then, we generate the direct context based on the user query and combine these two contexts to get a more relevant answer. This is considered to be the best approach for reasoning questions, as the model gets to explore a broader view of the user query and use it to compare with the specific context from the direct user query to get a more nuanced response. To summarize, this method involves abstraction and causal reasoning to make the model more generalized because it focuses on the underlying principles as well.\n",
    "\n",
    "For example, if we ask a question of the form \"Why does my LangGraph agent astream_events return {LONG_TRACE} instead of {DESIRED_OUTPUT}\" we will likely retrieve more relevant documents if we search with the more generic question \"How does astream_events work with a LangGraph agent\" than if we search with the specific user question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d8fb328-908d-4702-b883-dee27d7055a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv, dotenv_values\n",
    "import google.generativeai as genai\n",
    "from IPython.display import Markdown, display\n",
    "import os \n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "os.getenv(\"GOOGLE_API_KEY\") \n",
    "my_api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "genai.configure(api_key=my_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c934d5bb-bc90-4c93-8c7b-b8e080afd3ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    }
   ],
   "source": [
    "#### INDEXING ####\n",
    "\n",
    "# Load blog\n",
    "import bs4\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "blog_docs = loader.load()\n",
    "\n",
    "# Split\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter.from_tiktoken_encoder(\n",
    "    chunk_size=300, \n",
    "    chunk_overlap=50)\n",
    "\n",
    "# Make splits\n",
    "splits = text_splitter.split_documents(blog_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffdc6456-7be4-4675-b69e-cd094ea12023",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "## Call Embedding Model\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/text-embedding-004\")\n",
    "\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=splits, \n",
    "                                    embedding=embedding)\n",
    "\n",
    "retriever = vectorstore.as_retriever()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7d25427-40ec-4347-9628-3333f022fe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate, FewShotChatMessagePromptTemplate\n",
    "\n",
    "# LLM\n",
    "llm = ChatGoogleGenerativeAI(model= \"gemini-1.5-flash\", temperature = 0)\n",
    "\n",
    "\n",
    "examples = [\n",
    "    {\n",
    "        \"input\": \"Could the members of The Police perform lawful arrests?\",\n",
    "        \"output\": \"what can the members of The Police do?\",\n",
    "    },\n",
    "    {\n",
    "        \"input\": \"Jan Sindel’s was born in what country?\",\n",
    "        \"output\": \"what is Jan Sindel’s personal history?\",\n",
    "    },\n",
    "]\n",
    "# We now transform these to example messages\n",
    "example_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{output}\"),\n",
    "    ]\n",
    ")\n",
    "few_shot_prompt = FewShotChatMessagePromptTemplate(\n",
    "    example_prompt=example_prompt,\n",
    "    examples=examples,\n",
    ")\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\n",
    "            \"system\",\n",
    "            \"\"\"You are an expert at world knowledge. Your task is to step back and paraphrase a question to a more generic step-back question, which is easier to answer. Here are a few examples:\"\"\",\n",
    "        ),\n",
    "        # Few shot examples\n",
    "        few_shot_prompt,\n",
    "        # New question\n",
    "        (\"user\", \"{question}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fcc2f328-1c74-49e4-a160-8032ab571e5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How can complex tasks be broken down for AI agents? \\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_queries_step_back = prompt | llm | StrOutputParser()\n",
    "question = \"What is task decomposition for LLM agents?\"\n",
    "generate_queries_step_back.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3c5dba81-a337-4a23-8727-df81a9d92117",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnablePassthrough, RunnableLambda\n",
    "\n",
    "# Response prompt \n",
    "response_prompt_template = \"\"\"You are an expert of world knowledge. I am going to ask you a question. Your response should be comprehensive and not contradicted with the following context if they are relevant. Otherwise, ignore them if they are not relevant.\n",
    "\n",
    "# {normal_context}\n",
    "# {step_back_context}\n",
    "\n",
    "# Original Question: {question}\n",
    "# Answer:\"\"\"\n",
    "response_prompt = ChatPromptTemplate.from_template(response_prompt_template)\n",
    "\n",
    "chain = (\n",
    "    {\n",
    "        # Retrieve context using the normal question\n",
    "        \"normal_context\": RunnableLambda(lambda x: x[\"question\"]) | retriever,\n",
    "        # Retrieve context using the step-back question\n",
    "        \"step_back_context\": generate_queries_step_back | retriever,\n",
    "        # Pass on the question\n",
    "        \"question\": lambda x: x[\"question\"],\n",
    "    }\n",
    "    | response_prompt\n",
    "    |llm \n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "res = chain.invoke({\"question\": question})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd7c7b14-6b6b-467d-9a94-85042664858d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task decomposition is a crucial aspect of LLM-powered autonomous agents, enabling them to tackle complex tasks effectively. It involves breaking down a large, intricate task into smaller, more manageable subtasks. This approach allows the agent to handle each subtask individually, making the overall problem easier to solve.\n",
      "\n",
      "Here's how task decomposition works in LLM agents:\n",
      "\n",
      "* **Chain of Thought (CoT):** This prompting technique encourages the LLM to \"think step by step,\" decomposing a complex task into smaller, simpler steps. This approach helps the model utilize more test-time computation and provides insights into its reasoning process.\n",
      "* **Tree of Thoughts (ToT):** ToT extends CoT by exploring multiple reasoning possibilities at each step. It creates a tree structure by generating multiple thoughts for each subtask, allowing for a more comprehensive exploration of potential solutions.\n",
      "* **Methods for Task Decomposition:**\n",
      "    * **LLM Prompting:** Simple prompts like \"Steps for XYZ. \\n1.\" or \"What are the subgoals for achieving XYZ?\" can guide the LLM to decompose tasks.\n",
      "    * **Task-Specific Instructions:** Providing task-specific instructions, such as \"Write a story outline\" for writing a novel, can help the LLM understand the required steps.\n",
      "    * **Human Inputs:** Human input can be used to guide the decomposition process, providing the LLM with specific instructions or suggestions.\n",
      "\n",
      "By effectively decomposing tasks, LLM agents can:\n",
      "\n",
      "* **Improve Efficiency:** Breaking down complex tasks into smaller units makes them easier to manage and solve.\n",
      "* **Enhance Robustness:** Handling subtasks individually allows the agent to recover from errors more easily, making it more robust.\n",
      "* **Facilitate Interpretation:** The decomposition process provides insights into the LLM's reasoning, making it easier to understand its decision-making process.\n",
      "\n",
      "Overall, task decomposition is a fundamental technique for enabling LLM agents to handle complex tasks effectively. It allows them to break down problems into manageable chunks, improving efficiency, robustness, and interpretability. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0102670-b7b3-401d-b683-3f059d1f052a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
